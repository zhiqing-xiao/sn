# Notation

### General rules

- Upper-case letters are random events or random numbers, while lower-case letters are deterministic events or deterministic variables.
- The serif typeface, such as $X$, denotes numerical values. The sans typeface, such as $\mathsfit{X}$, denotes events in general, which can be either numerical or not numerical.
- Bold letters denote vectors (such as $\mathbf{w}$) or matrices (such as $\mathbf{F}$), where matrices are always upper-case, even they are deterministic matrices.
- Calligraph letters, such as $\mathcal{X}$, denote sets.
- Fraktur letters, such as *&#x1D523;*, denote mappings.

### Table

In the sequel are notations throughout the book. We also occasionally follow other notations defined locally.

| Latin Letters | Description |
| :---: | --- |
| $A$, $a$ | advantage |
| $\mathsfit{A}$, $\mathsfit{a}$ | action |
| $\mathcal{A}$ | action space |
| $B$, $b$ | behavior policy in off-policy learning; numerical belief in partially observable tasks; (lower case only) bonus |
| $\mathsfit{B}$, $\mathsfit{b}$ | belief in partially observable tasks |
| *&#x1D505;*$_\pi$, *&#x1D51F;*$_\pi$ | Bellman expectation operator of policy $\pi $ (upper case only used in distributional RL) |
| *&#x1D505;*$_\ast$, *&#x1D51F;*$_\ast$ | Bellman optimal operator (upper case only used in distributional RL) |
| $\mathcal{B}$ | a batch of transition generated by experience replay; belief space in partially observable tasks |
| $\mathcal{B}^+$ | belief space with terminal belief in partially observable tasks |
| $c$ | counting; coefficients in linear programming |
| $d$, $d_\infty$ | metrics |
| $d_f$ | $f$-divergence |
| $d_\text{KL}$ | KL divergence |
| $d_\text{JS}$ | JS divergence |
| $d_\text{TV}$ | total variation |
| $D_t$ | indicator of episode end |
| $\mathcal{D}$ | set of experience |
| $e$ | eligibility trace |
| $\text{E}$ | expectation |
| *&#x1D523;* | a mapping |
| $\mathbf{F}$ | Fisher information matrix |
| $G$, $g$ | return |
| $\mathbf{g}$ | gradient vector |
| $h$ | action preference |
| $\Eta$ | entropy |
| $k$ | index of iteration |
| $\ell$ | loss |
| $p$ | probability, dynamics |
| $\mathbf{P}$ | transition matrix |
| $o$ | observation probability in partially observable tasks |
| $\mathsfit{O}$, $\mathsfit{o}$ | observation |
| $\Pr$ | probability |
| $Q$, $q$ | action value |
| $Q_\pi$, $q_\pi$ | action value of policy $\pi$ (upper case only used in distributional RL) |
| $Q_\ast$, $q_\ast$ | optimal action values (upper case only used in distributional RL) |
| $\mathbf{q}$ | vector representation of action values |
| $R$, $r$ | reward |
| $\mathcal{R}$ | reward space |
| $\mathsfit{S}$, $\mathsfit{s}$ | state |
| $\mathcal{S}$ | state space |
| $\mathcal{S}^+$ | state space with terminal state |
| $T$ | steps in an episode |
| *&#x1D532;* | belief update operator in partially observable tasks |
| $U$, $u$ | TD target; (lower case only) upper bound |
| $V$, $v$ | state value |
| $V_\pi$, $v_\pi$ | state value of the policy $\pi $ (upper case only used in distributional RL) |
| $V_\ast$, $v_\ast$ | optimal state values (upper case only used in distributional RL) |
| $\mathbf{v}$ | vector representation of state values |
| $\mathrm{Var}$ | variance |
| $\mathbf{w}$ | parameters of value function estimate |
| $\mathsfit{X}$, $\mathsfit{x}$ | an event |
| $\mathcal{X}$ | event space |
| $\mathbf{z}$ | parameters for eligibility trace |
| **Greek Letters** | **Description** |
| $\alpha$ | learning rate |
| $\beta$ | reinforce strength in eligibility trace; distortion function in distributional RL |
| $\gamma$ | discount factor |
| $\mathit\Delta$, $\delta$ | TD error |
| $\varepsilon$ | parameters for exploration |
| $\lambda$ | decay strength of eligibility trace |
| $\mathit\Pi$, $\pi$ | policy |
| $\pi_\ast$ | optimal policy |
| $\pi_\Epsilon$ | expert policy in imitation learning |
| $\boldsymbol\uptheta$ | parameters for policy function estimates |
| $\vartheta$ | threshold for value iteration |
| $\rho$ | visitation frequency; important sampling ratio in off-policy learning |
| $\boldsymbol\uprho$ | vector representation of visitation frequency |
| $\huge\tau$, $\tau$ | sojourn time of SMDP |
| $\mathsfit{T}$, &#x1D7BD; | trajectory |
| $\mathit\Omega$, $\omega$ | accumulated probability in distribution RL; (lower case only) conditional probability for partially observable tasks |
| $\mathit\Psi$ | Generalized Advantage Estimate (GAE) |
| **Other Notations** | **Description** |
| $\le$ | compare numbers; element-wise comparison; partial order of policy |
| $\ll$ | absolute continuous |
| $\varnothing$ | empty set |
| $\nabla$ | gradient |
| $\sim$ | obey a distribution |
| $\left|\quad\right|$ | absolute value of a real number; element-wise absolute values of a vector or a matrix; the number of elements in a set |
